# nolint start: line_length_linter
#' Run automatic quality control on a deployment
#'
#' `qc_deployment()` will:
#' 1. Read the calibrated dissolved oxygen and conductivity files created by
#' HOBOware.
#' 2. Read and parse the associated Details files into nested lists.
#' 3. Reformat and merge the datafiles.
#' 4. Add automatic flags, and create columns for manual QC
#' 5. Write the data and metadata.
#'
#' `qc_deployment()` assumes the files are arranged in a specific
#' [file structure](https://docs.google.com/document/d/1kJttcEXzpNNknGwjkVwdYHw9LzZyjJ-FaX0CrU7H7NU/edit#heading=h.s6vs4d7vj0cn).
#'
#' @note Input files are looked for within `file.path(dir, "Calibration")`
#' using a heuristic that assumes the detail files end in `"Details.txt"`; the
#' data files end in `".csv"`; the dissolved oxygen files have `"DO_"`;
#' the Salinity/Conductivity files have either `"Cond_"`, `"Con_"` or `"Sal_`
#' in the name; and that there is only one of each kind of file - four files
#'  total.
#'
#' @param dir The path to a deployment directory to run QC on.  It should
#' have a `Calibration/` directory with calibrated DO and conductivity data
#' associated Details files generated by HOBOware.
#'
#' @return A list with items:
#' \item{d}{The final tabular data as written to disk.}
#' \item{md}{A nested list of the metadata.}
#' @export
#'
#' @examples
#' \dontrun{
#'   paths <- setup_example_dir()
#'   a <- qc_deployment(paths$deployment)
#' }
#'
#'
#'
qc_deployment <- function(dir){

  #============================================================================#
  # Setup                                                                   ####
  #============================================================================#

  #----------------------------------------------------------------------------#
  # Set Paths                                                               ####
  #----------------------------------------------------------------------------#

  # Standardize slashes
  dir <- gsub("[/\\\\]", .Platform$file.sep, dir)
  dir <- gsub("[/\\\\]$", "", dir) # drop trailing /


  if(!file.exists(dir))
    stop("Input deployment directory: ", dir, " does not exist.")

  # Set calibration directory
  cal_dir <- file.path(dir, "Calibrated")
  if(!dir.exists(cal_dir))
    stop("Expected to find calibration directory:", cal_dir, "\n")


  # Set input paths
  l <- list.files(cal_dir, full.names = TRUE)
  l <- gsub("\\\\", "/", l)
  paths = list(
    do = grep("DO_[^/]*\\.csv$", l, value = TRUE, ignore.case = TRUE),
    cond = grep("(Cond*|Sal)_[^/]*\\.csv$", l, value = TRUE, ignore.case = TRUE),
    do_details = grep("DO_[^/]*details\\.txt$", l, value = TRUE,
                      ignore.case = TRUE),
    cond_details = grep("(Cond*|Sal)_[^/]*details\\.txt$", l, value = TRUE,
                        ignore.case = TRUE))


  cat("Reading from calibration folder:\n  ", dirname(paths[[1]]), "\n",
      "Files:\n", sep = "")
  input_files <- paths |> unlist() |> basename()
  paste("  ", names(paths), ": ", input_files, "\n", collapse = "", sep = "") |>
    cat()
  rm(input_files)

  if(!all(sapply(paths, length) == 1))
    stop("Missing expected files in the calibration directory (\"",
         cal_dir, "\")", sep = "")

  # Extract information from directory structure
  # dir should be a deployment directory so should have this structure:
  # /BB_Data/<year>/<site>/<year>-<month>-<day>/
  dirs <- strsplit(dir, "[/\\\\]")[[1]]
  folder_date <- dirs[length(dirs)]
  site <- dirs[length(dirs) - 1]
  rm(dirs)

  # Set output paths
  out_paths <- list(
    auto_qc = file.path(dir, paste0("Auto_QC_", site, "_",
                                    folder_date, ".csv")),
    prelim_qc = file.path(dir, paste0("Preliminary_QC_", site, "_",
                                      folder_date, ".csv")),
    final_qc =  file.path(dir, paste0("QC_", site, "_",
                                      folder_date, ".csv")),
    metadata = file.path(dir, paste0("Metadata_", site, "_",
                                     folder_date, ".yml"))
  )

  old_output <- unlist(out_paths)[file.exists(unlist(out_paths))]
  if(length(old_output) > 0) {
    stop("Output already exists:\n  ",
         paste(old_output, collapse = "\n  "), sep = "")
  }

  #----------------------------------------------------------------------------#
  # Set Column names                                                        ####
  #----------------------------------------------------------------------------#

  # Expected column names in the merged logger data (after cleanup)
  expected_logger_cols <- c(
    "Date_Time",
    "Raw_DO",
    "Temp_DOLog",
    "DO",
    "DO_Pct_Sat",
    "Salinity_DOLog",
    "High_Range",
    "Temp_CondLog",
    "Spec_Cond",
    "Salinity",
    "Time")

  # Final columns names in order
  final_cols <- c(
    "Site",
    "Date",
    "Date_Time",
    "Gen_QC",
    "Flags",
    "Time",
    "Time_QC",
    "Temp_DOLog",
    "Temp_DOLog_Flag",
    "Temp_DOLog_QC",
    "Temp_CondLog",
    "Temp_CondLog_Flag",
    "Temp_CondLog_QC",
    "Raw_DO",
    "Raw_DO_Flag",
    "Raw_DO_QC",
    "DO",
    "DO_Flag",
    "DO_QC",
    "DO_Calibration_QC",
    "DO_Pct_Sat",
    "DO_Pct_Sat_Flag",
    "DO_Pct_Sat_QC",
    "Salinity_DOLog",
    "Salinity_DOLog_Flag",
    "Salinity_DOLog_QC",
    "Salinity",
    "Salinity_Flag",
    "Salinity_QC",
    "Sal_Calibration_QC",
    "High_Range",
    "High_Range_Flag",
    "High_Range_QC",
    "Spec_Cond",
    "Spec_Cond_Flag",
    "Spec_Cond_QC",
    "QA_Comment",
    "Field_Comment")

  #============================================================================#
  # Process Metadata                                                        ####
  #============================================================================#

  # From HOBOware Details.txt files
  md <- c(get_do_details(paths$do_details),
          get_cond_details(paths$cond_details))

  # Consolidate information from the two devices

  # Format timezone
  a <- md$do_device$header_created
  do_tz <- gsub("^.*(GMT-[[:digit:]]+:[[:digit:]])", "\\1", a)
  a <- md$cond_device$header_created
  cond_tz <-  gsub("^.*(GMT-[[:digit:]]+:[[:digit:]])", "\\1", a)
  stopifnot(do_tz == cond_tz)
  md$timezone <- do_tz
  rm(a, cond_tz, do_tz)

  # Deployment interval (minutes)
  do_interval <- md$do_deployment$logging_interval_min
  cond_interval <- md$cond_deployment$logging_interval_min
  if(!do_interval == cond_interval)
    stop("The logging interval for the two devices does not match. DO: ",
         do_interval, " min, Cond: ", cond_interval, sep = " min")
  md$logging_interval_min <- do_interval
  md$do_deployment$logging_interval <- NULL
  md$do_deployment$logging_interval_min <- NULL
  md$cond_deployment$logging_interval <- NULL
  md$cond_deployment$logging_interval_min <- NULL

  # Other information
  md$site <- site
  md$deployment <- paste0(site, "_", folder_date)
  #  md$deployment_date <- folder_date
  md$auto_qc_date <- lubridate::today() |> as.character()

  md_order <- c("site",  "deployment", "auto_qc_date", # "deployment_date",
                "logging_interval_min","timezone",
                "do_calibration", "do_deployment", "do_device", "cond_calibration",
                "cond_deployment", "cond_device")
  stopifnot(all(md_order %in% names(md)))

  md <- md[md_order]



  #============================================================================#
  # Process data
  #============================================================================#

  #----------------------------------------------------------------------------#
  # Read and Process Calibrated DO and Conductivity Tables                  ####
  #----------------------------------------------------------------------------#

  # Read tabular data
  do <- readr::read_csv(paths$do, col_types = readr::cols())
  cond <- readr::read_csv(paths$cond, col_types = readr::cols())

  # Extract serial number
  do_sn <- get_logger_sn(do)
  cond_sn <- get_logger_sn(cond)

  # Verify serial numbers
  if(do_sn != md$do_device$serial_number)
    stop("DO Serial number in csv does not match serial number in Details.txt")
  if(cond_sn != md$cond_device$serial_number)
    stop("Cond Serial number in csv does not match serial number in ",
         "Details.txt")

  #### Need to verify SN against site and placement tables  ####

  # Check and save observation interval


  # Preliminary column name cleanup
  do <- clean_logger_header(do)
  cond <- clean_logger_header(cond)

  # Rename identical columns to avoid name collisions
  do <- dplyr::rename(do, Temp_DOLog = "Temp", Salinity_DOLog = "Salinity")
  cond <- dplyr::rename(cond, Temp_CondLog = "Temp",
                        Salinity = "Salinity")

  # Merge
  d <- dplyr::full_join(do, cond, by = "Date_Time")

  #----------------------------------------------------------------------------#
  # Reformat - add and rename columns
  #----------------------------------------------------------------------------#

  #  "DO" to "Raw_DO", and "DO_Adj" to "DO"
  names(d)[names(d) == "DO"] <- "Raw_DO"
  names(d)[names(d) == "DO_Adj"] <- "DO"


  # Time column
  dates <- lubridate::mdy_hms(d$Date_Time)
  d$Time <- hms::as_hms(dates)
  rm(dates)

  # Check for missing columns
  missing_cols <- setdiff(expected_logger_cols, names(d))
  if(length(missing_cols) > 0)
    stop("The calibration data data is missing some expected columns: \"",
         paste(missing_cols, collapse = "\", \""), "\"", sep = "")

  # Check for extra columns
  extra_cols <- setdiff(names(d), final_cols)
  if(length(extra_cols) > 0)
    stop("There are unexpected extra columns in the calibrated logger data: \"",
         paste(extra_cols, collapse = "\", \""), "\"", sep = "")


  # Construct full data frame with all the expected columns
  full <- matrix(nrow = nrow(d), ncol = length(final_cols),
                 dimnames = list(NULL, final_cols)) |> as.data.frame()
  full[, names(d)] <- d

  d <- full

  # Add values to columns
  d$Site <- site
  d$Date <- lubridate::mdy_hms(d$Date_Time) |> lubridate::as_date()


  #============================================================================#
  # QC                                                                      ####
  #============================================================================#

  #----------------------------------------------------------------------------#
  # Check basic assumptions
  #----------------------------------------------------------------------------#

  # Check interval length
  temp_dt <- lubridate::mdy_hms(d$Date_Time)
  interval <- temp_dt[2] - temp_dt[1]
  units(interval) <- "mins"
  interval_min <- as.numeric(interval)
  rm(temp_dt)

  if(!interval_min == md$logging_interval_min)
    stop("Apparent interval from log (", interval_min," min)",
         " does not match interval from details (",
         md$logging_interval_min, " min).")

  if(!all(order(d$Date) == 1:nrow(d)))
    stop("Log is not in date-time order.")

  # Check that date in deployment folder name matches last date in data
  end_date <- max(d$Date)
  if(end_date != lubridate::as_date(folder_date))
    stop("Last date in log files: ", end_date,
            " does not match deployment date in path: ", date, sep = "")


  ### Check for right Site here???  (against placements table)


  if(FALSE){
    # Verification plots comparing the two loggers
    # I'm going to move this to the QC_Report
    with(d, plot(1:nrow(d), Temp_DOLog - Temp_CondLog))
    with(d, plot(1:nrow(d), DO_Salinity_ppt - Cond_Salinity_ppt))
  }

  #----------------------------------------------------------------------------#
  # Immediate rejection flags
  #----------------------------------------------------------------------------#

  # 1.	Immediate-rejection flags applied (coded with a final QC code of 9
  #   and skipping the human-review step)
  # a)	Any value of  -888.88  (sensor error indicated)
  # b)	Any temperature
  #              < 5°
  #              > 35°C
  # c)	Raw Conductivity High Range: <1,000
  # d)	Raw Conductivity High Range: >55,000 µS/cm
  # e)	Raw Dissolved Oxygen: >20 mg/L

  # What
  # TD = Temp, DO Logger
  # TC = Temp, Cond Logger
  # HR =  High Range
  # R = Raw DO,
  # D = DO,
  # S = Salinity

  # Error codes
  # e = logger error (immediate rejection)
  # l = low (immediate rejection low)
  # h = high (immediate rejection high)
  # sl = low for site
  # sh = high for site
  # pl = persistently low (low for more than an hour)
  # j = jumps
  # lv = low variation
  # ls = low streak


  # Check Temperature
  d$Temp_DOLog_Flag <- ir_check_temperature(d$Temp_DOLog, "D")
  d$Temp_CondLog_Flag<- ir_check_temperature(d$Temp_CondLog, "C")

  # Check conductivity high range
  d$High_Range_Flag <- ir_check_high_range(d$High_Range)
  ir_check_high_range(d$High_Range)

  # Check raw dissolved oxygen
  d$Raw_DO_Flag <- ir_check_raw_do(d$Raw_DO)

  # Update general flag to indicate immediate rejection
  d$Gen_QC[!d$Temp_DOLog_Flag == "" |
             !d$Temp_CondLog_Flag == "" |
             !d$High_Range_Flag == "" |
             !d$Raw_DO_Flag == ""]  <- 9


  #----------------------------------------------------------------------------#
  # Fouling flags (These trigger review)
  #----------------------------------------------------------------------------#

  # Fouling Flags
  #  a) if DO is <0.5 mg/L for more than an hour, that data is suspect  -- Dpl
  #  b) If adjusted DO (mg/l) jumps by > +/- 2.0 between two timepoints - Dj
  #  c) If salinity changes by > +/- 0.75 between two timepoints          Sj
  #  d) <0.01 variation for > 1 hour - if data essentially flatlines - Dlv, Clv
  #     need a flag/way to detect.
  #  e) salinity or DO are outside of values established for each site:
  #       Dsl, Dsh, Ssl, Ssh


  d$DO_Flag <- check_do(d$DO,
                        interval = md$logging_interval_min,
                        site = md$site)
  # Need to set md$site, and need to do site specific range check

  d$Salinity_Flag <- check_salinity(d$Salinity,
                                    interval = md$logging_interval_min,
                                    site = md$site)

  #----------------------------------------------------------------------------#
  # Finalize flags
  #----------------------------------------------------------------------------#

  flag_cols <- grep("flag", names(d), ignore.case = TRUE, value = TRUE)
  other_flag_cols <- setdiff(flag_cols, "Flags")

  # Concatenate all the flags into d$Flags
  d$Flags <- apply(
    d[, other_flag_cols], MARGIN = 1,
    FUN = function(x) paste(x[!is.na(x)], collapse = "", sep = ""))

  # Drop trailing ":" from flag columns
  for(col in flag_cols)
    d[[col]] <- gsub(":$", "", d[[col]])

  #----------------------------------------------------------------------------#
  # Update QC Code
  #----------------------------------------------------------------------------#

  # Don't overwrite preexisting (non-NA) immediate rejection flags
  d$Gen_QC[is.na(d$Gen_QC) & d$Flags != ""] <- 9999

  #----------------------------------------------------------------------------#
  # Write Files
  #----------------------------------------------------------------------------#

  readr::write_csv(d, out_paths$auto_qc, na = "")
  readr::write_csv(d, out_paths$prelim_qc, na = "")
  yaml::write_yaml(md, file = out_paths$metadata)

  cat("\n\nWriting to deployment folder:\n  ", dir,"\n",
      "Files:\n", sep = "")
  sel <- c("auto_qc", "prelim_qc", "metadata")
  cat(paste("  ", sel, ": ", basename(unlist(out_paths[sel])), "\n", sep = ""),
      sep = "")


  cat("\nReview and update QC codes in:\n  ", basename(out_paths$prelim_qc),
      "\n", "And rename to:\n  ", basename(out_paths$final_qc), "\n\n",
      sep = "")

  return(list(d = d, md = md))

}
# nolint end
